{"cells":[{"metadata":{},"cell_type":"markdown","source":"**INTRODUCTION**\n\nExploratory data analytics requires understanding of data and how different variables are correlated. \nTo understand the data using python data visualization and pandas we will be taking a case study of bank loan applications data. \n\nThe bank applications data has a dependent variable named TARGET variable whose value is 1 when the applicant is a defaulter and have missed the replayment of loan. For other applicants this value will be 0. All other columns in the data are considered independent variables. \n\nWe will explore the data step by step, visualize the data, create count/count percentage and summary tables and see how different variables like age, credit ratings, income etc of the applicant varies with the TARGET variable.\n\nThe following steps will be covered in the tutorial\n\n1.\tGetting the data ready<br><br>\n1.1\tUnderstanding the data<br>\n1.2\tCleaning data by dropping unwanted rows and columns<br>\n1.3\tHandling missing values<br>\n1.4\tHandling outliers<br>\n1.5\tChanging the data types of columns<br>\n1.6\tChanging column names into meaningful ones for analysis. <br>\n1.7\tCreating derived variables and binning the data. <br>\n1.8\tUnderstanding the data imbalance<br>\n\n2.\tData Analysis<br>\n2.1\tUnivariate Analysis<br>\n2.1.1\tCategorical<br>\n2.1.2\tNumerical<br>\n2.2\t  Bivariate  Analysis<br>\n2.2.1  Categorical  & Categorical<br>\n2.2.2\tNumerical & Numerical<br>\n2.2.3\tCategorical & Numerical<br>\n    \n**Note**\n* Kindly go through the code comments for details.\n* As we explore the data, I have added the Analysis and conclusion that we can make out with the plots and summary tables.\n* For the understanding of concepts, I have just taken 3 to 4 variables at a time for analysis . You can extend the analysis for other variables in the similar fashion.\n* This is just the beginning and not the exhaustive exploration. Feel free to leave suggestions in the comments section."},{"metadata":{},"cell_type":"markdown","source":"##  1. Getting the data ready\n\nThis is the first and most important step in data analysis. Usually there are large number of columns, redundant rows and missing values in a data. Its is very important to clean the data before performing any analysis.\n\n   ## 1.1 Understanding the data"},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt \nimport warnings\nwarnings.filterwarnings('ignore')\n\n%matplotlib inline\npd.set_option('display.max_rows', 500)\n#df = pd.read_csv(\"application_data.csv\")\ndf = pd.read_csv(\"/kaggle/input/application_data.csv\")\n#Sanity checks on the data\ndf.head()\ndf.info()\ndf.shape\n#df.dtypes","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 1.2\tCleaning data by dropping unwanted rows and columns"},{"metadata":{"trusted":true},"cell_type":"code","source":"# sum it up to check how many rows have all missing values\ndf.isnull().all(axis=1).sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# % of the missing values (column-wise)\ncol_missing_perc = round(100*(df.isnull().sum()/len(df.index)), 2) ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#getting cols with more than 20% missing and dropping them\ncol_missing_perc_greater_20 = []\nfor i in range(0,len(col_missing_perc)):\n    if col_missing_perc[i]>20:\n        col_missing_perc_greater_20.append(col_missing_perc.index[i])\n    \n#dropping cols with more than 20% missing\ndf.drop(col_missing_perc_greater_20, axis = 1,inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#remaining columns\ndf.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Of the remaining columns with < 20% missing data,a detailed analysis has been done to pick the below 24 columns for further analysis."},{"metadata":{"trusted":true},"cell_type":"code","source":"#subsetting the data\ndf=df[['SK_ID_CURR',\n'TARGET',\n'NAME_CONTRACT_TYPE',\n'CODE_GENDER',\n'CNT_CHILDREN',\n'AMT_INCOME_TOTAL',\n'AMT_CREDIT',\n'AMT_ANNUITY',\n'AMT_GOODS_PRICE',\n'NAME_INCOME_TYPE',\n'NAME_EDUCATION_TYPE',\n'NAME_FAMILY_STATUS',\n'NAME_HOUSING_TYPE',\n'DAYS_BIRTH',\n'DAYS_EMPLOYED',\n'CNT_FAM_MEMBERS',\n'ORGANIZATION_TYPE',\n'AMT_REQ_CREDIT_BUREAU_HOUR',\n'AMT_REQ_CREDIT_BUREAU_DAY',\n'AMT_REQ_CREDIT_BUREAU_WEEK',\n'AMT_REQ_CREDIT_BUREAU_MON',\n'AMT_REQ_CREDIT_BUREAU_QRT',\n'AMT_REQ_CREDIT_BUREAU_YEAR',\n'EXT_SOURCE_2'\n]]\n##final list of columns for analysis\ndf.columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#checking missing % in remaining columns\nround(100*(df.isnull().sum()/len(df.index)), 2)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 1.3\tHandling missing values\n\nThe below columns have been chosen to showcase the imputation of missing values from the subset of the data selected.\n\na) ORGANIZATION_TYPE - Unordered Categorical variable.<br>\nb) CODE_GENDER - Unordered Categorical variable.<br>\nc) CNT_FAM_MEMBERS - Ordered categorical variable.<br>\n\nFor the above variables,MODE has been used to impute the missing data as the data is categorical in nature and mode represents the most common category.\n       \nFor the continuous numerical value,MEAN or MEDIAN is usually used to impute the missing data. When the data is normally distributed without any large number of outliers, taking the mean would be the best option. But sometimes when the data is not normally distributed, example, there might be few applicants with extraordinary large income. In such cases the distribution will be skewed and taking MEDIAN to impute the missing value will be the best option."},{"metadata":{"trusted":true},"cell_type":"code","source":"\n#1.Handling missing values -  Categorical\n\ndf['ORGANIZATION_TYPE']=np.where(df['ORGANIZATION_TYPE'].isnull(),df['ORGANIZATION_TYPE'].mode(),df['ORGANIZATION_TYPE']) \ndf['CODE_GENDER']=np.where(df['CODE_GENDER']=='XNA',df['CODE_GENDER'].mode(),df['CODE_GENDER'])\n\n\n\ndf.loc[np.isnan(df['AMT_REQ_CREDIT_BUREAU_HOUR']), ['AMT_REQ_CREDIT_BUREAU_HOUR']] = df['AMT_REQ_CREDIT_BUREAU_HOUR'].median()\ndf.loc[np.isnan(df['AMT_REQ_CREDIT_BUREAU_DAY']),['AMT_REQ_CREDIT_BUREAU_DAY']]=df['AMT_REQ_CREDIT_BUREAU_DAY'].median()\ndf.loc[np.isnan(df['AMT_REQ_CREDIT_BUREAU_WEEK']),['AMT_REQ_CREDIT_BUREAU_WEEK']]=df['AMT_REQ_CREDIT_BUREAU_WEEK'].median()\ndf.loc[np.isnan(df['AMT_REQ_CREDIT_BUREAU_MON']),['AMT_REQ_CREDIT_BUREAU_MON']]=df['AMT_REQ_CREDIT_BUREAU_MON'].median()\ndf.loc[np.isnan(df['AMT_REQ_CREDIT_BUREAU_QRT']),['AMT_REQ_CREDIT_BUREAU_QRT']]=df['AMT_REQ_CREDIT_BUREAU_QRT'].median()\ndf.loc[np.isnan(df['AMT_REQ_CREDIT_BUREAU_YEAR']),['AMT_REQ_CREDIT_BUREAU_YEAR']]=df['AMT_REQ_CREDIT_BUREAU_YEAR'].median()\n\n#1.Handling missing values -  Numerical\ndf.loc[np.isnan(df['CNT_FAM_MEMBERS']),['CNT_FAM_MEMBERS']]=df['CNT_FAM_MEMBERS'].median()\ndf.loc[np.isnan(df['AMT_ANNUITY']),['AMT_ANNUITY']]=round(df['AMT_ANNUITY'].median(),1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#checking missing % in remaining columns\nround(100*(df.isnull().sum()/len(df.index)), 2)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<b>We still have 0.21% of EXT_SOURCE_2 missing. As it is very less percentage, the rows with null value of EXT_SOURCE_2 has been deleted. Also EXT_SOURCE_2 is the credit rating of an applicant and is an important column for performing analysis on it is better to delete rather than imputing with an incorrect value. "},{"metadata":{"trusted":true},"cell_type":"code","source":"df = df.dropna(axis=0, subset=['EXT_SOURCE_2'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"round(100*(df.isnull().sum()/len(df.index)), 2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### 0.09% of missing value of AMT_GOODS_PRICE will be handled after removal of outliers in the data."},{"metadata":{},"cell_type":"markdown","source":"## 1.4\tHandling outliers\n\nIdentification and handling of outliers has been done on the 3 columns - 'AMT_ANNUITY','AMT_GOODS_PRICE','AMT_CREDIT' for which the quantile range of 25-75% has been considered.\n\nIt is important to understand that extremely high value is not always outlier. In some cases extremely high or extremely low  value can indicate the missing information (eg: -999999 or 999999 in case of numerical data)\nHere in our case outliers have been chosen for removal in order to calculate mean value of columns. Ouliers in our case are rare occurances and may misrepresent the dataset if considered for analysis.\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Identifying and treating Outliers on columns - AMT_ANNUITY,AMT_GOODS_PRICE,AMT_CREDIT\n\n\ndf_outliers=df[['AMT_ANNUITY','AMT_GOODS_PRICE','AMT_CREDIT']]\n#df_outliers.shape (306574, 3)--before outlier removal\nQ1=df_outliers.quantile(0.25)\nQ3=df_outliers.quantile(0.75)\n\nIQR=Q3-Q1\nprint(IQR)\n\n#in case you decide to remove outliers, follow the below command.\ndf_out_final=df_outliers[~((df_outliers < (Q1-1.5*IQR)) | (df_outliers > ((Q3 + 1.5*IQR)))).any(axis=1)]\n\n\n# The mean value will be used further to impute missing value in the respective columns\ndf_out_final['AMT_GOODS_PRICE'].mean() \n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"len(df_out_final.index)/len(df.index)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"This shows that 7% of the data is having outliers for AMT_GOODS_PRICE and AMT_GOODS_PRICE. Therefore, these rows are not entirely deleted from the dataset and only missing values for AMT_GOODS_PRICE are imputed by calculating mean after removing the outliers."},{"metadata":{"trusted":true},"cell_type":"code","source":"#imputing missing value of AMT_GOODS_PRICE with mean of data after removing the outlier\ndf.loc[np.isnan(df['AMT_GOODS_PRICE']),['AMT_GOODS_PRICE']]=round(df_out_final['AMT_GOODS_PRICE'].mean(),1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## verification of the fixes\nround(100*(df.isnull().sum()/len(df.index)), 2)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### We can now confirm now that we do not have any missing values and continue further with analysis."},{"metadata":{},"cell_type":"markdown","source":"## 1.5\tChanging the data types of columns\n\nThe following columns had integer data but the datatype is float.The below code converts them to int."},{"metadata":{"trusted":true},"cell_type":"code","source":"#changing datatype of the columns\ndt_dict={'AMT_REQ_CREDIT_BUREAU_HOUR':int,\n        'CNT_FAM_MEMBERS':int,\n        'AMT_REQ_CREDIT_BUREAU_WEEK':int,\n        'AMT_REQ_CREDIT_BUREAU_MON':int,\n        'AMT_REQ_CREDIT_BUREAU_DAY':int,\n        'AMT_REQ_CREDIT_BUREAU_QRT':int,\n        'AMT_REQ_CREDIT_BUREAU_YEAR':int}\n\ndf=df.astype(dt_dict)\n\n\n# checking the datatypes\ndf.info()\n   ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 1.6\tChanging column names into meaningful ones for analysis."},{"metadata":{"trusted":true},"cell_type":"code","source":"#removing unnecessary spaces in column names\ndf.columns=[df.columns[i].strip() for i in range(len(df.columns))]\n\n#renaming columns\ndf.rename(columns={\"EXT_SOURCE_2\": \"CREDIT_RATINGS\"},inplace=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 1.7\tCreating derived variables and binning the data.\n\nTwo columns have been chosen for binning,namely\n\nDAYS_BIRTH<br>\nCREDIT_RATINGS\n\nFrom the DAYS_BIRTH, AGE of the client is calculated and then AGE_GROUPS are formed based on the age.\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Categorising customers into following\n#Youth (<18)\n#Young Adult (18 to 35)\n#Adult (36 to 55)\n#Senior (56 and up)\n\ndf['AGE'] = abs(df['DAYS_BIRTH'])\ndf['AGE'] = round(df['AGE']/365,1)\ndf['AGE']\n\ndf['AGE'].describe()\ndef age_group(y):\n    if y>=56:\n        return \"Senior\"\n    elif y>=36 and y<56:\n        return \"Adult\"\n    elif y>=18 and y<36:\n        return \"Young Adult\"\n    else:\n        return \"Youth\"\n    \ndf['AGE_GROUP'] = df['AGE'].apply(lambda x: age_group(x))\n\nsns.countplot(x='AGE_GROUP',hue='TARGET',data=df)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Just a quick analysis from above plot: Seniors for less likely to be a defaulter in replaying the loan."},{"metadata":{"scrolled":true},"cell_type":"markdown","source":"<b> Binning CREDIT_RATINGS based on quantiles </b>\n<br>\nThe credit rating is being categorized ino C1,C2,C3 and C4, where C1 category is the highest.\nThe categorization is done based on the quantiles."},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"df['CREDIT_RATINGS'].describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.boxplot(y=df['CREDIT_RATINGS'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"credit_category_quantile = list(df['CREDIT_RATINGS'].quantile([0.20,0.5,0.80,1]))\ncredit_category_quantile","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def credit_group(x):\n    if x>=credit_category_quantile[2]:\n        return \"C1\"\n    elif x>=credit_category_quantile[1]:\n        return \"C2\"\n    elif x>=credit_category_quantile[0]:\n        return \"C3\"\n    else:\n        return \"C4\"\ndf[\"CREDIT_CATEGORY\"] = df['CREDIT_RATINGS'].apply(lambda x: credit_group(x))\n\nsns.countplot(x='CREDIT_CATEGORY',hue='TARGET',data=df)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 1.8\tUnderstanding the data imbalance"},{"metadata":{"trusted":true},"cell_type":"code","source":"df['TARGET'].value_counts(normalize=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<b> There is an imbalance in the data where only 8% clients are with payment difficulties and 91% clients are all others."},{"metadata":{},"cell_type":"markdown","source":"## 2.\tData Analysis"},{"metadata":{},"cell_type":"markdown","source":"Before performing analysis, you need to identify the variables as categorical variable or numerical variable. "},{"metadata":{"trusted":true},"cell_type":"code","source":"#checking for unique values per column to see what all columns can be categorised\ndf.nunique().sort_values()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"From above we can see that there are variables above AGE can be considered as categorical variable (as unique values for them is very less compared to length of the data) and rest as contiuous variables."},{"metadata":{},"cell_type":"markdown","source":"#### Dividing the data based on the dependent variable (TARGET) for further analysis"},{"metadata":{"trusted":true},"cell_type":"code","source":"df0 = df[df['TARGET']==0]\ndf1 = df[df['TARGET']==1]\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 2.1 Univariate Analysis\nDone to check the impact of one independent variable on a dependent variable"},{"metadata":{"trusted":true},"cell_type":"code","source":"# What are average values of numerical features\ndf.pivot_table(columns = 'TARGET', aggfunc = 'median')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 2.1.1 Univariate Analysis for Categorical Variable\n\n    o\tNeed to check: Counts/Count% \n    o\tPlots: bar-charts, stacked bar charts\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"df['NAME_INCOME_TYPE'].unique()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(20,9))\nsns.countplot(x='NAME_INCOME_TYPE',hue='TARGET',data=df)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"##### As there is huge data imbalance, converting the numbers into percentages and using them for plots and analyzing"},{"metadata":{"trusted":true},"cell_type":"code","source":"incomeCategories0 = pd.DataFrame(df0['NAME_INCOME_TYPE'].value_counts().rename(\"Count_0\").reset_index())\nincomeCategories0_perct = pd.DataFrame(df0['NAME_INCOME_TYPE'].value_counts(normalize=True).rename(\"Perct_0\").reset_index())\nincomeCategories0.rename(columns={\"index\":\"NAME_INCOME_TYPE\"})\nincomeCategories0_perct.rename(columns={\"index\":\"NAME_INCOME_TYPE\"})\n\n#Merging data to get the overall view of the variable \"NAME_INCOME_TYPE\"\nincomeCategories0 = pd.merge(incomeCategories0,incomeCategories0_perct,how=\"inner\").rename(columns={\"index\":\"NAME_INCOME_TYPE\"})\nincomeCategories0\n\nincomeCategories1 = pd.DataFrame(df1['NAME_INCOME_TYPE'].value_counts().rename(\"Count_1\").reset_index())\nincomeCategories1_perct = pd.DataFrame(df1['NAME_INCOME_TYPE'].value_counts(normalize=True).rename(\"Perct_1\").reset_index())\nincomeCategories1.rename(columns={\"index\":\"NAME_INCOME_TYPE\"})\nincomeCategories1_perct.rename(columns={\"index\":\"NAME_INCOME_TYPE\"})\n\n#Merging data to get the overall view of the variable \"NAME_INCOME_TYPE\"\nincomeCategories1 = pd.merge(incomeCategories1,incomeCategories1_perct,how=\"inner\").rename(columns={\"index\":\"NAME_INCOME_TYPE\"})\nincomeCategories1\n\nincomeCategories = pd.merge(incomeCategories0,incomeCategories1,how=\"inner\").rename(columns={\"index\":\"NAME_INCOME_TYPE\"})\n\ndef income_percentage_contri_0(count_0, count_1):\n    return 100*(count_0/(count_0+count_1))\n\ndef income_percentage_contri_1(count_0, count_1):\n    return 100*(count_1/(count_0+count_1))\n\nincomeCategories['percentage_contri_0'] = incomeCategories[['Count_0','Count_1']].apply(lambda x: income_percentage_contri_0(*x), axis=1)\nincomeCategories['percentage_contri_1'] = incomeCategories[['Count_0','Count_1']].apply(lambda x: income_percentage_contri_1(*x), axis=1)\nincomeCategories.set_index(\"NAME_INCOME_TYPE\",inplace=True)\nincomeCategories","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"For the above table the column description is:\n    \n    Count_0 = Total number of non defaulters applicants which belong to that particular income type.\n              Ex: There are total 143550 non defaulter applicants that have \"Working\" as their income type.\n    \n    Count_1 = Total number of defaulter applicants which belong to that particular income time.\n              Ex: There are total 15224 defaulter applicants that have \"Working\" as their income type.\n    \n    Perct_0 = How much percentage the particular category contibutes to all the non defaulter applicants. Target = 0\n              Ex: Out of total applicants of Target=0, 50% of them have \"Working\" as their income type.\n    \n    Perct_1 = How much percentage the particular category contibutes to all the defaulter applicants. Target=1\n              Ex: Out of total applicants of Target=1, 61% of them have \"Working\" as their income type.\n    \n    percentage_contri_0 = Out of all the applicants with the particular category, how much percentage belong to Target=0\n              Ex: Out of total working applicants, 90% are those with Target =0 \n    \n    percentage_contri_1 = Out of all the applicants with the particular category, how much percentage belong to Target=1\n              Ex: Out of total working applicants, 9.5% are those with Target =1.\n              \n              \n#### Similar table with these columns will be used for analysis of other variables\n            "},{"metadata":{"trusted":true},"cell_type":"code","source":"fig = plt.figure(figsize=(20,5))\nplt.subplot(1,2,1)\nplt.title(\"Target = 1\")\nplt.ylabel('Percentage contribution to \"defaulters\"')\nplt.plot(incomeCategories['percentage_contri_1'])\n#ax1.set_xticklabels(labels = ax1.get_xticklabels(),rotation=30)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.rcParams.update(plt.rcParamsDefault)\nincomeCategories = incomeCategories.sort_values(by='percentage_contri_1')\nincomeCategories[['percentage_contri_1', 'percentage_contri_0']].plot(kind='bar', stacked=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Analysis:\nIgnoring Income type \"Unemployed\" and \"Maternity Leave\" as the data is very less.\n\nThe percentage of working people is maximum among the applicants.\n\nOut of income type \"Working\", \"Commercial associate\",\"Pensioners\" and \"state_servants\", Pensioners are highly likely to repay their loans on time.\n\nOut of all pensioners who applied for the loan, there is <b>94.6% </b>chance that he will repay the loan and <b>5.3% </b>that he will default.\n\nSimilarly out of all the working applicants, there is <b>90.4%</b> chance that he will repay the loan and <b>9.5%</b> chance that he will default.\n\n#### Therefore, the total impact of income type on the defaulters is 4.2% (9.58% being the max and 5.3% being the worst.)\n#### So if we want to consider applicants, applicants with income type as pensioners should be given the highest priority."},{"metadata":{},"cell_type":"markdown","source":"<b> Let us consider an another variable categorical variable CODE_GENDER - the gender of the applicant and see how the gender of a person impacts the target variable. "},{"metadata":{"trusted":true},"cell_type":"code","source":"df.CODE_GENDER.unique()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.countplot(x='CODE_GENDER',hue='TARGET',data=df)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"genderCategories0 = pd.DataFrame(df0['CODE_GENDER'].value_counts().rename(\"Count_0\").reset_index())\ngenderCategories0_perct = pd.DataFrame(df0['CODE_GENDER'].value_counts(normalize=True).rename(\"Perct_0\").reset_index())\ngenderCategories0.rename(columns={\"index\":\"CODE_GENDER\"})\ngenderCategories0_perct.rename(columns={\"index\":\"CODE_GENDER\"})\n\n#Merging data to get the overall view of the variable \"NAME_INCOME_TYPE\"\ngenderCategories0 = pd.merge(genderCategories0,genderCategories0_perct,how=\"inner\").rename(columns={\"index\":\"CODE_GENDER\"})\ngenderCategories0\n\ngenderCategories1 = pd.DataFrame(df1['CODE_GENDER'].value_counts().rename(\"Count_1\").reset_index())\ngenderCategories1_perct = pd.DataFrame(df1['CODE_GENDER'].value_counts(normalize=True).rename(\"Perct_1\").reset_index())\ngenderCategories1.rename(columns={\"index\":\"CODE_GENDER\"})\ngenderCategories1_perct.rename(columns={\"index\":\"CODE_GENDER\"})\n\n#Merging data to get the overall view of the variable \"NAME_INCOME_TYPE\"\ngenderCategories1 = pd.merge(genderCategories1,genderCategories1_perct,how=\"inner\").rename(columns={\"index\":\"CODE_GENDER\"})\ngenderCategories1\n\ngenderCategories = pd.merge(genderCategories0,genderCategories1,how=\"inner\").rename(columns={\"index\":\"CODE_GENDER\"})\n\ndef gender_percentage_contri_0(count_0, count_1):\n    return 100*(count_0/(count_0+count_1))\n\ndef gender_percentage_contri_1(count_0, count_1):\n    return 100*(count_1/(count_0+count_1))\n\ngenderCategories['percentage_contri_0'] = genderCategories[['Count_0','Count_1']].apply(lambda x: gender_percentage_contri_0(*x), axis=1)\ngenderCategories['percentage_contri_1'] = genderCategories[['Count_0','Count_1']].apply(lambda x: gender_percentage_contri_1(*x), axis=1)\ngenderCategories.set_index(\"CODE_GENDER\",inplace=True)\ngenderCategories","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.rcParams.update(plt.rcParamsDefault)\ngenderCategories = genderCategories.sort_values(by='percentage_contri_1')\ngenderCategories[['percentage_contri_1', 'percentage_contri_0']].plot(kind='bar', stacked=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Analysis:\n\nOut of all the Females who have applied, 93% of them have repayed their loan and 7% of them  are defaulters.\n\nAnd if a male candidate applies, there is a 10% chance that he will default.\n\n<b> Gender can decrease the total % of \"defaulters\" by -3.24%. \n We can say that Females are likely to repay their loans on time"},{"metadata":{},"cell_type":"markdown","source":"## 2.1.2 Univariate Analysis for Numerical Variable\n        o\tNeed to check: mean, median, mode, min, max,range, quantiles, standard deviation.\n        o\tPlots: Distribution, histogram, Box Plots\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"df1['CREDIT_RATINGS'].describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df0['CREDIT_RATINGS'].describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.barplot(x=\"TARGET\",y=\"CREDIT_RATINGS\",data=df)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.boxplot(x=\"TARGET\", y=\"CREDIT_RATINGS\", data=df,palette='rainbow')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<b>Quick Analysis: </b>\nThe median of credit ratings of defaulters tends to be lower than that of non defaulter applicants.\nAs we see that upper quantile of defaulters overlaps with the lower quantile of non defaulters, they still have tendency to repay but are defaulting. Therefore credit rating is not the only factor affecting the rate of defaulters. We need to consider other factors also."},{"metadata":{"trusted":true},"cell_type":"code","source":"target =[0,1]\nfor i in target:\n    subset = df[df['TARGET'] == i]\n    sns.distplot(subset['CREDIT_RATINGS'],hist=False,kde=True,kde_kws ={'shade':True},label=i)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Distribution of '0' is skewed. Applicants with high credit ratings, tends to repay their loan.\nApplicants with lower credit score tends to have a larger defaulter rate. "},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(20,9))\nplt.subplot(1,2,1)\ndf0['CREDIT_RATINGS'].hist(bins = 50)\nplt.subplot(1,2,2)\ndf1['CREDIT_RATINGS'].hist(bins = 50)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<b> Quick Analysis</b>\n\n    There are more random peeks to be observed in above plot with target = 1 as compared to those with Target= 0. \n    The detailed analysis of categories of credit rating will be done in segmented univariate analaysis with CREDIT_CATEGORY"},{"metadata":{},"cell_type":"markdown","source":"Next step is to check the various categories of credit ratings CREDIT_CATEGORY. (We previously binned the credit ratings into categories C1,C3,C3,C4 with C1 being the highest."},{"metadata":{"trusted":true},"cell_type":"code","source":"df['CREDIT_CATEGORY'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"creditCategories0 = pd.DataFrame(df0['CREDIT_CATEGORY'].value_counts().rename(\"Count_0\").reset_index())\ncreditCategories0_perct = pd.DataFrame(df0['CREDIT_CATEGORY'].value_counts(normalize=True).rename(\"Perct_0\").reset_index())\ncreditCategories0.rename(columns={\"index\":\"CREDIT_CATEGORY\"})\ncreditCategories0_perct.rename(columns={\"index\":\"CREDIT_CATEGORY\"})\n\n#Merging data to get the overall view of the variable \"NAME_INCOME_TYPE\"\ncreditCategories0 = pd.merge(creditCategories0,creditCategories0_perct,how=\"inner\").rename(columns={\"index\":\"CREDIT_CATEGORY\"})\ncreditCategories0\n\ncreditCategories1 = pd.DataFrame(df1['CREDIT_CATEGORY'].value_counts().rename(\"Count_1\").reset_index())\ncreditCategories1_perct = pd.DataFrame(df1['CREDIT_CATEGORY'].value_counts(normalize=True).rename(\"Perct_1\").reset_index())\ncreditCategories1.rename(columns={\"index\":\"CREDIT_CATEGORY\"})\ncreditCategories1_perct.rename(columns={\"index\":\"CREDIT_CATEGORY\"})\n\n#Merging data to get the overall view of the variable \"NAME_INCOME_TYPE\"\ncreditCategories1 = pd.merge(creditCategories1,creditCategories1_perct,how=\"inner\").rename(columns={\"index\":\"CREDIT_CATEGORY\"})\ncreditCategories1\n\ncreditCategories = pd.merge(creditCategories0,creditCategories1,how=\"inner\").rename(columns={\"index\":\"CREDIT_CATEGORY\"})\n\ndef credit_percentage_contri_0(count_0, count_1):\n    return 100*(count_0/(count_0+count_1))\n\ndef credit_percentage_contri_1(count_0, count_1):\n    return 100*(count_1/(count_0+count_1))\n\ncreditCategories['percentage_contri_0'] = creditCategories[['Count_0','Count_1']].apply(lambda x: credit_percentage_contri_0(*x), axis=1)\ncreditCategories['percentage_contri_1'] = creditCategories[['Count_0','Count_1']].apply(lambda x: credit_percentage_contri_1(*x), axis=1)\ncreditCategories.set_index(\"CREDIT_CATEGORY\",inplace=True)\ncreditCategories","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.plot(creditCategories['percentage_contri_1'].sort_values())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"creditCategories[['percentage_contri_1', 'percentage_contri_0']].plot(kind='bar', stacked=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"  <b>Analysis</b>:\n  \n    There tends to be a inverse relationship between the credit rating and the defaulters, with C1 being the highest credit rating group, there are less percentage of defaulters.\n\n    If we consider an applicant from C1 category credit rating, there is only 3.5% chance that he will be a defaulter and if we consider an applicant with C4 category, there is 15% chance that he will be a defaulter.\n\n    Therefore the credit rating can decrease the percentage of defaulters by (15%- 3.5%) 11.5%.\n    So if we want to consider applicants, applicants with higher credit ratings should be given the highest priority."},{"metadata":{},"cell_type":"markdown","source":"## 2.2 Bivariate Analysis \nDone to check the impact of two independent variables on a dependent variable."},{"metadata":{},"cell_type":"markdown","source":"   ## 2.2.1 Categorical  & Categorical\n        o\tNeed to check: Counts/Count% \n        o\tPlots: Bar chart, Stacked bar chart, 2-y Axis plot – line charts \n"},{"metadata":{"trusted":true},"cell_type":"code","source":"pt = df.pivot_table(columns='NAME_INCOME_TYPE',index='CREDIT_CATEGORY',values='TARGET',aggfunc='sum',fill_value = 0)\n#pt.reset_index()\n\npt","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pt['Row_Total'] = pt['Businessman'] + pt['Commercial associate'] + pt['Maternity leave'] + pt['Pensioner']+pt['State servant'] +pt['Student']+pt['Unemployed']+pt['Working']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Column_Total = []\nfor c in pt.columns:\n    Column_Total.append(pt[c].sum())\nColumn_Total\npt.loc['Column_Total'] = Column_Total\npt","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for i in pt.index:\n    pt.loc[i,'Total%'] = 100*(pt.loc[i,'Row_Total']/pt.loc['Column_Total','Row_Total'])\n\nfor j in df.NAME_INCOME_TYPE.unique():\n    for i in pt.index:\n        pt.loc[i,j+'%'] = 100*(pt.loc[i,j]/pt.loc['Column_Total',j])\npt","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"credit_income_type = pt.iloc[0:-1][['Working%','State servant%','Commercial associate%','Pensioner%','Unemployed%']]\ncredit_income_type\ncredit_income_type.T.plot.bar(stacked = 'TRUE')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"    Analysis:\n    Of all the C1 credit rating category applicants, the maximum percentage of defaulters are 'Commmercial Associates'\n    The Percentage of applicants with income type as 'working' and with C1 credit rating is the lowest in all the defaulters.\n    "},{"metadata":{},"cell_type":"markdown","source":"## 2.2.2 Numerical & Numerical\n\n    o\tNeed to check: correlations\n    o\tPlots: heatmaps, scatter plots, hex plots\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"df1_corr=df[df['TARGET']==1]\ndf0_corr=df[df['TARGET']==0]\n\ndf1_corr=df1_corr[[\n'AMT_INCOME_TOTAL',\n'AMT_CREDIT',\n'AMT_ANNUITY',\n'AMT_GOODS_PRICE',\n'AGE',\n'DAYS_EMPLOYED']]\n\ndf0_corr=df0_corr[[\n'AMT_INCOME_TOTAL',\n'AMT_CREDIT',\n'AMT_ANNUITY',\n'AMT_GOODS_PRICE',\n'AGE',\n'DAYS_EMPLOYED']]\n\ndf1_corr_matrix=df1_corr.corr()\ndf0_corr_matrix=df1_corr.corr()\ndf1_corr_matrix","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\n#narrowing down the data and considering less than the upper quantile AMT_INCOME_TOTAL\ndf1_corr['AMT_INCOME_TOTAL'] = df1_corr[df1_corr['AMT_INCOME_TOTAL']<df1_corr['AMT_INCOME_TOTAL'].quantile(.85)]['AMT_INCOME_TOTAL']\n#df1_corr['AMT_ANNUITY'] = df1_corr[df1_corr['AMT_GOODS_PRICE']<df1_corr['AMT_GOODS_PRICE'].quantile(.85)]['AMT_GOODS_PRICE']\n\nfig, ax = plt.subplots(figsize=(10,10)) \nsns.scatterplot(x='AMT_INCOME_TOTAL', y='AMT_ANNUITY',data=df1_corr)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df1_corr.plot.hexbin(x='AMT_INCOME_TOTAL', y='AMT_ANNUITY', gridsize=30)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Analysis:\n> From the above hexplot we can see that large number of application lie in the region where amount annuity tends to be between 20% to 30% of the applicants income\n    "},{"metadata":{"trusted":true},"cell_type":"code","source":"#narrowing down the data and considering less than the upper quantile AMT_INCOME_TOTAL\ndf1_corr['AMT_CREDIT'] = df1_corr[df1_corr['AMT_CREDIT']<df1_corr['AMT_CREDIT'].quantile(.85)]['AMT_CREDIT']\n#df1_corr['AMT_ANNUITY'] = df1_corr[df1_corr['AMT_GOODS_PRICE']<df1_corr['AMT_GOODS_PRICE'].quantile(.85)]['AMT_GOODS_PRICE']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df1_corr.plot.hexbin(x='AGE', y='AMT_CREDIT', gridsize=15)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":" **Analysis:\n>    From the above hexplot we can see that applicants with age 28 to 37 tend to get the larger amount of credit as compared to older applicants. We can easily confirm this with the following box plot.\n    "},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.boxplot(x=\"AGE_GROUP\", y=\"AMT_CREDIT\", data=df1,palette='rainbow')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The median of AMT_CREDIT tends to highest in Adults and lowest in Young Adults. "},{"metadata":{},"cell_type":"markdown","source":"<b>Finding correlations and heatmaps for all the continous variables in your data. Follow the below steps to calculate correlations between all the numerical variables in the data.\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"df1_corrdf = df1_corr_matrix.where(np.triu(np.ones(df1_corr_matrix.shape),k=1).astype(np.bool))\ndf0_corrdf = df0_corr_matrix.where(np.triu(np.ones(df0_corr_matrix.shape),k=1).astype(np.bool))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df1_corrdf = df1_corrdf.unstack().reset_index()\ndf0_corrdf = df0_corrdf.unstack().reset_index()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df1_corrdf.columns =['var1','var2','correlation']\ndf0_corrdf.columns=['var1','var2','correlation']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df1_corrdf.dropna(subset=['correlation'],inplace=True)\ndf0_corrdf.dropna(subset=['correlation'],inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df1_corrdf.sort_values(by=['correlation'],ascending=False)\n#df0_corrdf.sort_values(by=['correlation'],ascending=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Analysis:**\n> From the above table we can easily see that AMT_GOODS_PRICE and  AMT_CREDIT have the highest corelation, which is quite obvious as higher the good price, higher will be the loan value.\n    "},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.heatmap(df1_corr_matrix,annot=True,linewidth=1,annot_kws={\"size\":10},cbar=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 2.2.3 Categorical & Numerical\n   * Plots: Box plots, line chart\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"#removing outlier for AMT_INCOME_TOTAL\ndf1_filtered = df1[df1['AMT_INCOME_TOTAL']<df['AMT_INCOME_TOTAL'].quantile(.90)]\ndf_stats_credit = df1_filtered.groupby('NAME_INCOME_TYPE').mean()[['AMT_CREDIT', 'AMT_INCOME_TOTAL', 'AMT_GOODS_PRICE']]\n#df_stats_credit = df1_filtered.groupby('AGE_GROUP').mean()[['CREDIT_RATINGS']]\ndf_stats_credit.sort_values(by='AMT_CREDIT',ascending=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_stats_credit.plot.line(x_compat=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(10,5))\ndf_filtered = df[df['AMT_INCOME_TOTAL']<df['AMT_INCOME_TOTAL'].quantile(.90)]\n\nsns.boxplot(x=\"NAME_CONTRACT_TYPE\", y=\"AMT_CREDIT\", data=df_filtered,palette='rainbow',hue='TARGET')\nplt.yscale(\"log\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Analysis:** \n\nFrom the above box plots,we can infer that people with defaulting intentions tend to take less credit of revolving loans as revolving loans involve repayment and then reusing it.\nHence, we can deduce that more scrunity can be put in place for cash loans as against to Revolving loans."},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.1"}},"nbformat":4,"nbformat_minor":1}